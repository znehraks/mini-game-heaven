# claude-symphony AI Benchmarking Configuration
# Model performance comparison and dynamic selection

benchmarking:
  enabled: true
  description: "AI model performance comparison and optimal model dynamic selection"

  # Stages where benchmarking is enabled
  enabled_stages:
    - "06-implementation"
    - "07-refactoring"
    - "09-testing"

  # Benchmark task definitions
  benchmark_tasks:
    code_generation:
      description: "Code generation quality comparison"
      models: ["claude", "codex"]
      metrics:
        - name: "correctness"
          weight: 0.4
          measure: "test_pass_rate"
        - name: "performance"
          weight: 0.2
          measure: "execution_time"
        - name: "style_compliance"
          weight: 0.2
          measure: "lint_score"
        - name: "readability"
          weight: 0.2
          measure: "complexity_score"

    refactoring:
      description: "Refactoring quality comparison"
      models: ["codex", "claude"]
      metrics:
        - name: "complexity_reduction"
          weight: 0.3
          measure: "cyclomatic_complexity_delta"
        - name: "test_coverage"
          weight: 0.3
          measure: "coverage_percentage"
        - name: "performance_improvement"
          weight: 0.2
          measure: "benchmark_time_delta"
        - name: "maintainability"
          weight: 0.2
          measure: "maintainability_index"

    test_generation:
      description: "Test code generation comparison"
      models: ["codex", "claude"]
      metrics:
        - name: "coverage"
          weight: 0.4
          measure: "line_coverage"
        - name: "edge_cases"
          weight: 0.3
          measure: "branch_coverage"
        - name: "quality"
          weight: 0.3
          measure: "mutation_score"

  # Auto-selection strategy
  selection_strategy:
    auto: true
    method: "weighted_score"

    thresholds:
      auto_select_minimum: 0.75  # Minimum score
      confidence_gap: 0.15       # Score gap between 1st and 2nd place

    fallback:
      enabled: true
      fallback_to: "claude"
      on_tie: "prefer_faster"

  # History tracking
  history_tracking:
    enabled: true
    storage_path: "state/ai_benchmarks/"
    retention_days: 90

    track_metrics:
      - "model"
      - "task_type"
      - "scores"
      - "execution_time"
      - "token_usage"
      - "timestamp"

    aggregation:
      period: "weekly"
      metrics: ["avg_score", "success_rate", "avg_latency"]

# Dynamic model selection settings
dynamic_selection:
  enabled: true
  description: "Automatic selection of optimal AI model based on task characteristics"

  criteria:
    task_type:
      weight: 0.4
      mapping:
        brainstorming: "gemini"
        implementation: "claude"
        refactoring: "codex"
        testing: "codex"
        research: "claude"

    complexity:
      weight: 0.3
      levels:
        low:
          prefer: "haiku"  # Fast response
          threshold: 100   # lines of code
        medium:
          prefer: "claude"
          threshold: 500
        high:
          prefer: "claude"  # Complex logic
          threshold: null

    previous_performance:
      weight: 0.3
      lookback_period: 7  # days
      min_samples: 3

  override:
    user_preference: true
    stage_assignment: true  # Prioritize stage_assignments in models.yaml

# Benchmarking execution settings
execution:
  parallel_benchmark: true
  max_concurrent_benchmarks: 2

  sample_tasks:
    enabled: true
    sample_size: 3
    sampling_method: "stratified"

  timeout:
    per_model: 300  # 5 minutes
    total: 900      # 15 minutes

  resource_limits:
    max_tokens_per_benchmark: 10000
    max_retries: 2

# Result reporting
reporting:
  format: "markdown"
  output_path: "state/ai_benchmarks/reports/"

  include:
    - summary_table
    - metric_breakdown
    - historical_comparison
    - recommendation

  visualization:
    enabled: true
    charts:
      - "score_comparison_bar"
      - "trend_line"
      - "radar_chart"

  notifications:
    on_significant_change:
      threshold: 0.2  # 20% change
      channels: ["console", "handoff"]

# Integration settings
integration:
  # Include benchmark results in HANDOFF
  handoff:
    include_summary: true
    include_recommendation: true

  # Benchmark data handling during context compression
  context_compression:
    preserve_recent: 3  # Preserve last 3 benchmark results
    summarize_older: true
